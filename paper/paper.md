# Big Data and Streaming :smiley: fa18-523-62 fa18-523-69

| Manek Bahl, Sohan Udupi Rai
| mbahl@iu.edu, surai@iu.edu
| Indiana University
| hid: fa18-523-62 fa18-523-69

| paper: [:cloud:](https://github.com/cloudmesh-community/fa18-523-62/blob/master/paper/paper)

**:mortar_board: Learning Objectives** 


## Abstract

In this research paper we discuss stream processing as a big data
technology. Data obtained in real-time from various sources are called Data
Streams; processing and extracting insights from such sources is called Big Data
Streaming or Real-time streaming analytics. While conventional big-data systems
can handle near real-time data by performing micro-batch processing, they have
still got latency issues which is not acceptable in some critical applications.
We look at various technologies available today for handling data streams and
see how each of these deal with the challenges associated with the task. The
paper then looks at some real-life examples exploring the implementation of Big
data streaming systems in various domains.

## Introduction

Deriving insights from data has always been the key requirement for
organizations to gain edge over competitors in the market. But there are certain
applications where this needs to be done within a few milliseconds for it to be
useful. Recent boom in the field of Internet of Things makes it paramount for
analytics systems to be able to deal with such huge data quickly and
effectively. This is where Big Data Streaming has gained immense importance in
the recent past.  

Various domains such as eCommerce, Banking and Finance, Social
media generate such data sequentially, and there is a heavy need for this data
to be processed as they arrive on a row by row basis to derive actionable
insights. These insights enable the companies to understand the recent consumer
behavior and act accordingly to cater to their needs promptly in a timely
manner. This beneficial both for the consumers, who get better service, and for
organizations, to be more efficient and proactive in the business decision
making [@fa18-523-62-aws-amazon-streaming-data].

## Stream Processing vs Batch Processing

For a long time, the traditional way of dealing with data has been Batch
Processing, wherein, the data is collected and continuously stored in a
database. Insights are derived from this data by dividing it into batches and
then deploying suitable algorithms. The size of the batch could be anything from
as short as a day to maybe even a year, depending on the type of insights
required. While this type of insight is valuable for the companies to make long
term business decisions, there are certain critical applications such as fraud
detection, which require analytics to be done in real-time so action can be
taken immediately, before the fraudulent transaction can be completed. There
arises the need for Stream Processing [@fa18-523-62-medium-gowthamy].  
Advantages of Stream Processing compared to Batch Processing:  
1) Data collected from continuous data sources such as
traffic sensors, transaction logs and most other IOT sensors is Time Series data
and Batch Processing would be tedious since there would be a need for
aggregation across batches. Stream processing handles the data without the need
for any such aggregation.  
2) Stream processing reduces the storage requirement of a system. Since analysis 
is done in real-time, there is no need for the entire generated data to be 
stored in the system. Only the useful data can be stored.  
3) Stream Processing requires less hardware capabilities compared to
Batch Processing since the amount of data on which analysis is performed is
small compared to the large volume of data in Batch Processing [@fa18-523-62-
medium-stream-processing].

## Challenges in Stream Processing

Unlike batch processing, wherein all of the incoming data is first stored and
then divided into batches for processing, stream processing systems must have
the capability to handle incoming data as and when it arrives. This leaves room
for Data loss. Another factor that comes into play when dealing Stream data is
ensuring data serialization. i.e. systems must ensure that messages must be
processed in the order in which they were generated by the source. Thus,
maintaining data integrity poses a great challenge for Stream processing
systems.  
Streaming application need to maintain the state and offset to keep
track of the last message that was processed. While this is relatively simple to
do, it poses a big challenge when dealing with system failure or in the case of
vertical scaling since the new version of the application may pose compatibility
issues [@fa18-523-62-codelook].

## Big Data Streaming Architecture and Technologies

### Apache Spark

Apache Spark is an open source analytics engine that uses distributed cluster
computing framework. It was developed in UC, Berkeley and the codebase was then
given to Apache Software Foundation which now maintains it. Spark is known for
its high performance and can be used interactively using Scala, R, Python and
SQL. It provides an interface to perform various analytics functions such as
Machine Learning using MLLib, Streaming using Spark Streaming and Exploratory
Analysis using GraphX. Spark Streaming enables us to build streaming data
applications which are scalable, fault tolerant. Spark has the capability to
ingest data from various sources such as HDFS, Kafka, Flume etc. and then uses
complex algorithms to process the data which can be then stored into a database
or can be written out to a file. Spark Streaming receives divides the incoming
data stream into batches which are then processed by the Spark Engine to produce
a batch of final data. Spark Streaming brings in an ease of operation as it lets
us write the streaming jobs in the same way batch jobs. The biggest advantage of
using Streaming is that it has the capability to recover lost work as an inbuilt
functionality. Moreover, the streaming data can be concatenated with historical
data to build interactive applications. Spark uses HDFS or ZooKeeper for high
availability. Currently Spark Streaming is widely being used by Yahoo, Uber,
Netflix and eBay for providing real time analytics [@fa18-523-62-spark-apache].

### Apache Storm

Storm is commonly known as the “Hadoop of Real Time Processing” as it processes
the streaming data as reliably as Hadoop does for batch processing. It is a
distributed framework used for stream processing which was developed by a team
at BackType and Twitter and was written in Clojure. The application is designed
at a directed acyclic graph with spouts (input streams) and bolts (processing
modules) as the vertices and the data flows in the form of tuples. The function
of the spouts is to get the input streaming data and pass it to the bolts.
Specialized spouts are available to retrieve data from multiple sources however
Storm provides an option to create customer spouts as well. The data is then
processed in the bolts and as per requirement the processed data is passed to a
database or a file system [@fa18-523-62-www-infoworld]. Like Spark, Storm also 
provides high fault-tolerance, but it does not have a feature to use the same 
code for batch and stream processing. Current users of Storm include Twitter, 
Groupon, Yahoo and Spotify for its ability to create low latency distributed 
systems for streaming.

### Apache Flink

Apache Flink uses a DataFlow model similar to Storm. But the main difference
which makes it superior is that unlike Storm, it processes the events as and
when they occur rather than processing micro-batches. This is especially useful
in applications where the data stream is sporadic i.e. extremely sparse. This
approach reduces amount resources needed to handle the stream. It also
eliminates the effort needed to determine the optimal size of micro-batches
which is done by trial and error in Storm. Flink is also more flexible compared
to Storm due to the simplistic nature of its API. Its SQL API interface makes it
very possible for non-programmers to deal with the data stream application.
Flink can be setup in either of the two modes – Standalone or Distributed
[@fa18-523-62-hackernoon-flink].


### Apache Kafka

Apache Kafka is an open source platform for Stream processing. Its design is
inspired from the Transaction logs maintained by a database management system.
It is a distributed system, wherein multiple nodes known as ‘Brokers’ work
together to form a cluster. Its distributed nature ensures high availability and
High scalability. Which means that Kafka systems are fault tolerant and provide
horizontal-scalability. Horizontal-scalability nature of Kafka is important
since it ensures that additional computing power can be provided to the system
by adding nodes without disrupting the ongoing operations. The main data
structure used by Kafka is a commit log data structure which only allows
appends. Once added to the data structure, records cannot be deleted or
modified. This ensures that the data are placed in exactly the same order in
which the events occurred. This data structure has an added advantage that reads
and writes are done in constant time O(1) which drastically increases the speed
at which Kafka can handle streaming. Read and write operations can be done
simultaneously as there is no ‘lock’ placed on the data during a write
operation.  The messages stored in the Kafka nodes are divided into sub-
divisions called Topics. Topics are further divided into smaller divisions
called Partitions, for improved performance. The commit log-type data structure
used in Kafka ensures that all messages in each partition are in the order in
which they came in. It also means that Kafka provides excellent performance
delivering messages at near network speed without placing the data in RAM, it
uses disks to store all its records instead.   Kafka stores the data for a set
amount of time during which the consumer can use offsets to pull any record they
want. Partitions are replicated and placed in multiple nodes of the system to
ensure High-availability [@fa18-523-62-hackernoon-Kafka].

### Amazon Kinesis

Kinesis is Amazon’s solution to streaming data and analytics. Kinesis Data
Stream is one of the key components of Kinesis platform which also includes the
Kinesis Video Streams, Kinesis Data Firehose and Kinesis Data Analytics. Data 
streams reads the data from various input sources in the form of data records. 
These data records can be analyzed or stored in whichever way the application 
demands. The data streams start ingesting data within a second of when the data 
is added. The data then can be sent to any of the built-integrations or can be 
stored in various third-party stores such as DynamoDB or Cassandra. One advantage 
that Kinesis Streams offers is scaling of the applications, it allows dynamic
changing of the throughput of the stream based on the volume of data expected 
[@fa18-523-62-aws-amazon-kinesis].

### Hortonworks Dataflow

Hortonworks DataFlow is an open source distributed platform capable of
ingesting, storing and analyzing streaming data from multiple sources
simultaneously. It provides a GUI, thus eliminating the need for the end user to
have an understanding of programming. Being powered by Apache Nifi, HDF has the
ability to ingest data from a variety of Streaming sources with ease. HDFs
integration with Apache Ranger ensures excellent data security. HDF uses Apache
Kafka as the streaming platform enabling it to process several million
transactions per second while allowing users to deploy several Machine learning
algorithms. The Streaming Analytics Manager provides users to perform the
analytics in a simple visual fashion [@fa18-523-62-hortonworks-hdf].

## Industrial Use Cases of Big Data Streaming

**E-Commerce:** Real time transactions can be clustered and used along-with 
various other features such as product reviews to provide recommendations to the
customers based to the latest trends. Alibaba and e-Bay are pioneers of using
big data streaming to enhance their business.

**Healthcare:** Streaming has become an integral part of healthcare industry now
with providers analyzing patient records to forecast any future health issues
and recommendations to prevent them. Companies such as MyFitnessPal use Apache
Spark to clean the data entered by the users to recommend healthy food diet.

**Entertainment:** Netflix’s application monitoring system is largely built on
Amazon Kinesis which monitors all the applications within Netflix and tries to
detect issues to give a high availability to its customers. Netflix also uses
Apache Spark to collect all user activities on the app and analyzes it to come
up with personalized recommendations.

**Social Media:** With various organizations paying so much attention to reviews
being posted by users of various Social Websites, it has become increasingly
important to be able to analyze the public sentiment. Twitter uses Apache
Storm internally on its various applications for anomaly detection to provide
high availability to its users.

**Banking and Finance:** One major use case of big data streaming in financial
domain is fraud detection. If a fraudulent transaction is detected, it is
imperious that corrective or preventive measures be taken in real time
[@fa18-523-62-www-dezyre-use-case].

## Conclusion

It  can  be  seen  that  systems  have  evolved  from  being  reliant  on
primitive  methods  such  as  micro-batch  processing  to  true  real-time
processing  by  adjusting  the  underlying  data  structures  being  used.
Transaction-log  type  of  architecture  have  gained  prominence,  consequently
reducing  the  need  for  large  amounts  of  data  being  stored  in
databases.  Hence  these  newer  stream  processing  technologies  such  as
Spark  Stream  and  Apache  Kafka  also  have  minimal  storage  and  processing
power  requirements  while  providing  features  such  as  horizontal
scalability  and  high  availability.

